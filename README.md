# Efficiently_Serving_LLMs
LLM Serving refers to the process of deploying and running large language models (LLMs) to handle user requests. It involves taking an LLM, which is typically trained offline, and setting it up to respond to queries in real-time.
![](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSKz9pXYJ2UJfYZomKgxUVg2r38QqSJ9_IDag&usqp=CAU)

![](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSN1II7V4Ra3l7N86_tZyWRAbfcxegt5mS52A&usqp=CAU)
Predibase is focused on individuals and organizations who have tried operationalizing machine learning, but found themselves re-inventing the wheel each step of the way. Our declarative approach allows users to focus on the “what” of their ML tasks while leaving our system to figure out the “how”. In this post, we’ll share more about our journey, the value we bring to both large Fortune 500 enterprises and high-growth startups, and what’s coming up next.

Predibase offers state-of-the-art fine-tuning techniques out of the box such as quantization, low-rank adaptation, and memory-efficient distributed training to ensure your fine-tuning jobs are fast and efficient—even on commodity GPUs.
